---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r,setup=TRUE}
library(tidyverse)
library(rstan)
library(nimble)
library(bayesplot)
```


# The Model

Let $\hat{\beta_{ij}}$ be the log response ratio estimate for study j in pico group i, where
\begin{align}
i = 1,\dots, m ,
j = 1,\dots, n_i,
\end{align}
where m is the number of pico comparison groups and $n_i$ is the number of publications in each comparison group.

In our model, the log response ratio estimate $\hat{\beta_{ij}}$ is modelled by
\begin{align}
\hat{\beta_{ij}} = \beta_i + \gamma_{ij} + \epsilon_{ij},
\end{align}
where $\beta_i$ is the log response ratio for pico comparison i, $\gamma_{ij}$ is design bias in study j in pico comparison i and $\epsilon_{ij}$ is the random error in study j in pico comparison i. These components are in turn modelled by:
\begin{align}
\beta_j &\sim \mathcal{N}(0,\sigma_\beta^2), \\
\gamma_{ij} &\sim \mathcal{N}(0,\sigma^2(X_{ij})), \\
\epsilon_{ij} &\sim \mathcal{N}(0,\sigma_{ij}^2).
\end{align}
where $X_{ij}$ is a vector of design features of study j in group i, $\sigma_{ij}$ is the standard error, and
\begin{align}
\log \sigma^2(X_{ij}) = X_{ij}^T \theta .
\end{align}
We also assume weakly informative prior distributions:
\begin{align}
\sigma_\beta \sim \text{InvGamma}(1,0.02), \\
\theta_i \sim \mathcal{N}(0,1.52) .
\end{align}

For now we are interested in the following design features:
- Randomisation,
- Controls,
- Before-after,
- Presence of all three previous features, i.e. BACI design.
So we set:
\begin{align}
X_{ij} = \begin{pmatrix} 1 \\
x_\text{randomised} \\
x_\text{controlled} \\
x_\text{before-after} \\
x_\text{BACI} \\
\end{pmatrix},
\end{align}
where the first element of the vector is the intercept, and $x_.$ is a dummy coding variable for the features we are considering.

# Constructing the model in Stan

```{stan output.var="model"}
data {
  int<lower=0> m; // number of pico comparisons
  int<lower=0> n[m]; // array of studies in each comparison
  int<lower=0> n_tot; // total number of studies in the model
  vector[n_tot] log_response; // log responses for all studies
  vector[n_tot] est_var; // estimated variance for all studies
  int<lower=0> d; // number of columns in X
  matrix[n_tot,d] X; // design matrix
}

parameters {
  vector[m] beta; // vector of log responses for the pico comparisons
  real<lower=0> sigma_beta; // standard deviation for each of the pico comparisons
  vector[d] theta; // vector of coefficients for the transformed linear relation between variance of gamma_ij and X_ij
}

model {
  int k; // this is an indexing variable for later
  
  sigma_beta ~ inv_gamma(1,0.02); // prior for sigma_beta
  
  beta ~ normal(0,sigma_beta); // prior for beta
  
  theta ~ normal(0,1.52); // prior for theta
  
  k = 1;
  for(i in 1:m){
    for(j in 1:n[i]){
      log_response[k] ~ normal(beta[i],sqrt(est_var[k] + exp(X[k,]*theta))); // relation between estiated log response and other random quantities
      k = k+1;
    }
  }
}


```

# Model checking via simulation

To check that the model is functioning as intended, we simulate some data to compare the model parameter estimates to the known values.

Setting the number of pico groups simulated:

```{r}
m <- 1000
```

We use a $\text{Pois}(1)+2$ to produce group sizes since this produces random group sizes of at least 2 which is the minimum group size required and mean 3, which was the ideal group size.

```{r}
n <- rpois(m,1) + 2

n_tot <- sum(n) #we also need the total number of groups
```

When running the simulation on the target data, $\sigma_\beta$ was estimated to be 0.36, so let us set $\sigma_\beta$ for the simulation to be a similar value, say 0.31:

```{r}
sigma_beta <- 0.31
```

Our model assumes $\beta_i \sim \mathcal{N}(0,\sigma_\beta)$:

```{r}
beta <- rnorm(m,0,sigma_beta)
```

Setting values for $\theta$ (which we shall later compare the model results to):

```{r}
theta <- c(-2,
           -1,
           -0.6,
           2,
           -1)
```

To generate $X_{ij}$, we randomly generate some attributes for each of the studies using a $\text{Bern}(0.5)$. We add the column for the intercept. Since for the original data, we required that each comparison group have at least one random study, we then set the first study in each comparison group to be randomised. Finally, we add the column for the interaction term of before-after and controlled, i.e. precense of BACI design.

```{r}
X <- matrix(sample(0:1, 3*n_tot, replace = TRUE),n_tot,3) #randomly generating study attributes
X <- cbind(rep(1,n_tot),X) #adding intercept
for(i in 1:length(n)){
  X[sum(n[1:i]),2] <- 1 #
}
X <- cbind(X,X[,3]&X[,4])
head(X)
```

We use $\sigma_{ij} \sim \text{InvGamma}(4,1)$ in order to create a sensible looking distribution of standard deviations for the studies:

```{r}
sigma_ij <- rinvgamma(n_tot,4,1)

summary(sigma_ij^2)
summary(exp(X %*% theta))
```

Finally we create $\hat{\beta}_{ij}$ using our model assumptions that $\gamma_{ij} \sim \mathcal{N}(0,\sigma(X_{ij}))$ and $\epsilon_{ij} \sim \mathcal{N}(0,\sigma_{ij})$:

```{r}
beta_hat <- rep(0,n_tot) #creating the vector of simulated values
k <- 1 #indexing variable
for(j in 1:m){ #looping through each pico comparison for the value of beta_i
  for(i in 1:n[j]){ #looping through each study
    beta_hat[k] <- beta[j] + rnorm(1,0,sqrt(exp(X[k,]%*%theta)+sigma_ij[k]^2))
    k <- k + 1
  }
}
```

Finally, we compile all the simulated data into the format required by the Stan model:

```{r}
stan_data <- list(
  m = m,

  n = as.array(n), #this is the number of distinct publications in each group, and is inputted as an array as this is the variable type of n in the model

  n_tot = n_tot, #this is the total number of publications in each group

  log_response = beta_hat, #this is the vector of log response ratios

  est_var = sigma_ij^2, #this is the vector of variances for the log response ratio

  d = 5, #specifies number of comparisons we are interested in (including the intercept)

  #Design matrix:
  X = X
)
```

Fitting the model to the simulated data:

```{r}
fit <- sampling(model,,data = stan_data)

print(fit, paste0("theta[", 1:5, "]"))
```
The estimated values for theta are relatively close to the values set in the simulation, thus we can be fairly confident that the model is functioning as expected.

# Fitting the model to the data

The comparison group design is determined using linear optimisation techniques:

```{r}
load("stan_data.Rdata")
design_frame_345
```

In this next step, we are adding the relevant `log_response_ratio`, `selected_v` and `Design` information to the design frame:

```{r}
test_design <- design_frame_345 %>%
  select(pico_id,pub_id,rowid) #selecting the relevant information from the design frame
test_design <- merge(test_design,select(Metadataset_download_19_08_21,rowid,log_response_ratio,selected_v,Design),) #adding information on the log response rato, variance and Designt to the design frame
test_design <- test_design %>%
  arrange(pico_id) #arranging by pico_id
test_design
```
Using the `test_design` we can now record data for the Stan model:

```{r}
stan_data <- list(
  m = length(unique(test_design$pico_id)), #since m is the number of pico comparisons
  
  n = as.array((test_design %>%
                  group_by(pico_id) %>%
                  summarise(num_publications = n_distinct(pub_id)))$num_publications), #this is the number of distinct publications in each group, and is inputted as an array as this is the variable type of n in the model
  
  n_tot = sum(as.array((test_design %>%
                      group_by(pico_id) %>%
                      summarise(num_publications = n_distinct(pub_id)))$num_publications)), #this is the total number of publications in each group
  
  log_response = as.vector(test_design$log_response_ratio), #this is the vector of log response ratios
  
  est_var = as.vector(test_design$selected_v), #this is the vector of variances for the log response ratio
  
  d = 5, #specifies number of comparisons we are interested in (including the intercept)
  
  #Design matrix:
  X = as.matrix(test_design %>%
                  select(Design) %>%
                  mutate(randomised = str_detect(Design,"(R|r)andom"),
                         controlled = str_detect(Design,"(C|c)ontrol"),
                         before_after = str_detect(Design,"(B|b)efore"),
                         BACI = controlled & before_after) %>% #creating columns containing boulean values indicating presence of design features of interest
                  select(-Design) %>% #removing un-needed columns
                  mutate(across(.cols = everything(),~if_else(.,1,0))) %>% #converting the bouleans into binary variables
                  mutate(intercept = 1) %>% #adding an intercept column
                  select(intercept,randomised,controlled,before_after,BACI))
) #rearranging for desired order of columns
```

Now we fit the Stan model to the data:

```{r}
fit <- sampling(model,data = stan_data)
print(fit, paste0("theta[", 1:5, "]"))
```
The chains appear to converge:

```{r,fig.width=20,fig.height=20}
fit %>%
  mcmc_trace()
```

# Discussion of results

From these estimates of theta, we can deduce the following relationship between study design and bias:

```{r}
posterior_means <- get_posterior_mean(fit)

study_design_bias <- data.frame(Design = c("BA","R-BA","CI","R-CI","BACI","R-BACI"),
                                log_sigma_beta = c(posterior_means["theta[1]",5] + posterior_means["theta[4]",4],
                                                   posterior_means["theta[1]",5] + posterior_means["theta[2]",5] + posterior_means["theta[4]",4],
                                                   posterior_means["theta[1]",5] + posterior_means["theta[3]",4],
                                                   posterior_means["theta[1]",5] + posterior_means["theta[2]",5] + posterior_means["theta[3]",4],
                                                   posterior_means["theta[1]",5] + posterior_means["theta[3]",4] + posterior_means["theta[4]",4] + posterior_means["theta[5]",4],
                                                   posterior_means["theta[1]",5] + posterior_means["theta[2]",5] + posterior_means["theta[3]",4] + posterior_means["theta[4]",4] + posterior_means["theta[5]",4]))

study_design_bias
```

According to these results, it seems that CI and R-CI are less prone to bias than the other study designs. BA is highly prone to bias.